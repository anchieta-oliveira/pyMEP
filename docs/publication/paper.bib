@article{Thain:2005,
  	url = {https://onlinelibrary.wiley.com/doi/10.1002/cpe.938},
  	Archiveprefix = {CCPE},
  	Author = {{Thain}, D. and {Tannenbaum}, T. and {Livny}, M.},
  	Eprint = {},
  	Journal = {Concurrency and Computation: Practice and Experience},
  	Keywords = {Condor - Grid - history; community - planning; scheduling - split execution},
  	Month = feb,
  	Title = {{Distributed computing in practice: the Condor experience}},
  	Year = 2005
}

@InProceedings{Iserte:2014,
  author={Iserte, Sergio and Castelló, Adrián and Mayo, Rafael and Quintana-Ortí, Enrique S. and Silla, Federico and Duato, Jose and Reaño, Carlos and Prades, Javier},
  booktitle={2014 IEEE 26th International Symposium on Computer Architecture and High Performance Computing}, 
  title={SLURM Support for Remote GPU Virtualization: Implementation and Performance Study}, 
  year={2014},
  volume={},
  number={},
  pages={318-325},
  keywords={Graphics processing units;Virtualization;Throughput;Middleware;Resource management;Computer architecture;Acceleration;HPC cluster;job scheduler;resource management;remote GPU virtualization},
  doi={10.1109/SBAC-PAD.2014.49}
}

@InProceedings{Yoo:2003,
  author="Yoo, Andy B.
  and Jette, Morris A.
  and Grondona, Mark",
  editor="Feitelson, Dror
  and Rudolph, Larry
  and Schwiegelshohn, Uwe",
  title="SLURM: Simple Linux Utility for Resource Management",
  booktitle="Job Scheduling Strategies for Parallel Processing",
  year="2003",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="44--60",
  abstract="A new cluster resource management system called Simple Linux Utility Resource Management (SLURM) is described in this paper. SLURM, initially developed for large Linux clusters at the Lawrence Livermore National Laboratory (LLNL), is a simple cluster manager that can scale to thousands of processors. SLURM is designed to be flexible and fault-tolerant and can be ported to other clusters of different size and architecture with minimal effort. We are certain that SLURM will benefit both users and system architects by providing them with a simple, robust, and highly scalable parallel job execution environment for their cluster system.",
  isbn="978-3-540-39727-4"
}

@InProceedings{Varrette:2022,
  author={Varrette, Sebastien and Kieffer, Emmanuel and Pinel, Frederic},
  booktitle={2022 21st International Symposium on Parallel and Distributed Computing (ISPDC)}, 
  title={Optimizing the Resource and Job Management System of an Academic HPC & Research Computing Facility}, 
  year={2022},
  volume={},
  number={},
  pages={129-137},
  keywords={Iris;Analytical models;Processor scheduling;Atmospheric measurements;Computational modeling;Particle measurements;Supercomputers;Slurm;Fairsharing;Workload analysis},
  doi={10.1109/ISPDC55340.2022.00027}}

@InProceedings{Georgiou:2013,
  author="Georgiou, Yiannis
  and Hautreux, Matthieu",
  editor="Cirne, Walfredo 
  and Desai, Narayan
  and Frachtenberg, Eitan
  and Schwiegelshohn, Uwe",
  title="Evaluating Scalability and Efficiency of the Resource and Job Management System on Large HPC Clusters",
  booktitle="Job Scheduling Strategies for Parallel Processing",
  year="2013",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="134--156",
  abstract="The Resource and Job Management System (RJMS) is the middleware in charge of delivering computing power to applications in HPC systems. The increasing number of computational resources in modern supercomputers brings new levels of parallelism and complexity. To maximize the global throughput while ensuring good efficiency of applications, RJMS must deal with issues like manageability, scalability and network topology awareness. This paper is focused on the evaluation of the so-called RJMS SLURM regarding these issues. It presents studies performed in order to evaluate, adapt and prepare the configuration of the RJMS to efficiently manage two Bull petaflop supercomputers installed at CEA, Tera-100 and Curie. The studies evaluate the capability of SLURM to manage large numbers of compute resources and jobs as well as to provide an optimal placement of jobs on clusters using a tree interconnect topology. Experiments presented in this paper are conducted using both real-scale and emulated supercomputers using synthetic workloads. The synthetic workloads are derived from the ESP benchmark and adapted to the evaluation of the RJMS internals. Emulations of larger supercomputers are performed to assess the scalability and the direct eligibility of SLURM to manage larger systems.",
  isbn="978-3-642-35867-8"
  }
